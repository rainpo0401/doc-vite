import{_ as t,c as a,o as e,a4 as i}from"./chunks/framework.DpC1ZpOZ.js";const h=JSON.parse('{"title":"Documentation","description":"","frontmatter":{},"headers":[],"relativePath":"comfyui/ADE/ADE_AdjustWeightIndivAttnAdd.md","filePath":"comfyui/ADE/ADE_AdjustWeightIndivAttnAdd.md"}'),n={name:"comfyui/ADE/ADE_AdjustWeightIndivAttnAdd.md"},l=i(`<h1 id="documentation" tabindex="-1">Documentation <a class="header-anchor" href="#documentation" aria-label="Permalink to &quot;Documentation&quot;">​</a></h1><ul><li>Class name: WeightAdjustIndivAttnAddNode</li><li>Category: Animate Diff 🎭🅐🅓/ad settings/weight adjust</li><li>Output node: False</li><li>Repo Ref: <a href="https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved.git" target="_blank" rel="noreferrer">https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved.git</a></li></ul><p>WeightAdjustIndivAttnAddNode 类旨在调整神经网络模型中各个注意力机制的权重。它提供了一种方法来微调注意力参数，例如查询（q）、键（k）和值（v）向量，以及输出权重和偏差。此节点允许根据特定用例或实验要求修改这些参数，以定制模型行为。</p><h1 id="input-types" tabindex="-1">Input types <a class="header-anchor" href="#input-types" aria-label="Permalink to &quot;Input types&quot;">​</a></h1><h2 id="required" tabindex="-1">Required <a class="header-anchor" href="#required" aria-label="Permalink to &quot;Required&quot;">​</a></h2><ul><li>pe_ADD <ul><li>pe_ADD 参数用于调整模型的位置编码权重。它在模型解释序列顺序方面起着至关重要的作用，这可能显著影响模型在对输入数据顺序敏感的任务上的性能。</li><li>Comfy dtype: FLOAT</li><li>Python dtype: float</li></ul></li><li>attn_ADD <ul><li>attn_ADD 参数允许调整模型内的一般注意力权重。这有助于强调或淡化输入数据的某些方面，从而影响模型的焦点，并可能增强其捕捉相关信息的能力。</li><li>Comfy dtype: FLOAT</li><li>Python dtype: float</li></ul></li><li>attn_q_ADD <ul><li>attn_q_ADD 参数专门针对注意力机制的查询权重。通过微调此参数，可以引导模型更多地关注某些输入特征，这对于需要深入理解输入上下文的任务特别有用。</li><li>Comfy dtype: FLOAT</li><li>Python dtype: float</li></ul></li><li>attn_k_ADD <ul><li>attn_k_ADD 参数负责调整注意力机制的键权重。修改此参数可以改变模型与输入数据的相关部分对齐的能力，这对于依赖准确上下文对齐的任务至关重要。</li><li>Comfy dtype: FLOAT</li><li>Python dtype: float</li></ul></li><li>attn_v_ADD <ul><li>attn_v_ADD 参数影响注意力机制内的值权重。它对于确定每个输入元素对最终输出的贡献很重要，这对于需要精确表示输入数据的任务至关重要。</li><li>Comfy dtype: FLOAT</li><li>Python dtype: float</li></ul></li><li>attn_out_weight_ADD <ul><li>attn_out_weight_ADD 参数用于调整注意力机制的输出权重。这有助于提炼模型的输出，使其更贴近期望的结果，这对于需要输出层高精度的任务特别重要。</li><li>Comfy dtype: FLOAT</li><li>Python dtype: float</li></ul></li><li>attn_out_bias_ADD <ul><li>attn_out_bias_ADD 参数允许调整注意力机制内的输出偏差。这对于调整模型的预测以更好地符合预期结果很有用，特别是对于需要精确输出调整的任务。</li><li>Comfy dtype: FLOAT</li><li>Python dtype: float</li></ul></li><li>other_ADD <ul><li>other_ADD 参数为模型内其他未指定的权重提供了一般性调整。它可以用来对模型的行为进行广泛的调整，这些调整不属于其他参数的具体类别。</li><li>Comfy dtype: FLOAT</li><li>Python dtype: float</li></ul></li><li>print_adjustment <ul><li>print_adjustment 参数是一个布尔标志，设置为 True 时启用记录权重调整的详细信息。这对于调试和理解调整如何影响模型参数很有帮助。</li><li>Comfy dtype: BOOLEAN</li><li>Python dtype: bool</li></ul></li></ul><h2 id="optional" tabindex="-1">Optional <a class="header-anchor" href="#optional" aria-label="Permalink to &quot;Optional&quot;">​</a></h2><ul><li>prev_weight_adjust <ul><li>prev_weight_adjust 参数是可选的先前权重调整组，可以应用于模型。这允许继续一系列调整或应用一组预定义的调整。</li><li>Comfy dtype: WEIGHT_ADJUST</li><li>Python dtype: Union[AdjustGroup, None]</li></ul></li></ul><h1 id="output-types" tabindex="-1">Output types <a class="header-anchor" href="#output-types" aria-label="Permalink to &quot;Output types&quot;">​</a></h1><ul><li>weight_adjust <ul><li>weight_adjust 输出提供了将各个注意力调整应用于模型的结果。它包含了所有输入参数对模型权重的综合影响，提供了调整后权重的结构化表示。</li><li>Comfy dtype: WEIGHT_ADJUST</li><li>Python dtype: AdjustGroup</li></ul></li></ul><h1 id="usage-tips" tabindex="-1">Usage tips <a class="header-anchor" href="#usage-tips" aria-label="Permalink to &quot;Usage tips&quot;">​</a></h1><ul><li>Infra type: CPU</li></ul><h1 id="source-code" tabindex="-1">Source code <a class="header-anchor" href="#source-code" aria-label="Permalink to &quot;Source code&quot;">​</a></h1><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>class WeightAdjustIndivAttnAddNode:</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    @classmethod</span></span>
<span class="line"><span>    def INPUT_TYPES(s):</span></span>
<span class="line"><span>        return {&#39;required&#39;: {&#39;pe_ADD&#39;: (&#39;FLOAT&#39;, {&#39;default&#39;: 0.0, &#39;min&#39;: -2.0, &#39;max&#39;: 2.0, &#39;step&#39;: 1e-06}), &#39;attn_ADD&#39;: (&#39;FLOAT&#39;, {&#39;default&#39;: 0.0, &#39;min&#39;: -2.0, &#39;max&#39;: 2.0, &#39;step&#39;: 1e-06}), &#39;attn_q_ADD&#39;: (&#39;FLOAT&#39;, {&#39;default&#39;: 0.0, &#39;min&#39;: -2.0, &#39;max&#39;: 2.0, &#39;step&#39;: 1e-06}), &#39;attn_k_ADD&#39;: (&#39;FLOAT&#39;, {&#39;default&#39;: 0.0, &#39;min&#39;: -2.0, &#39;max&#39;: 2.0, &#39;step&#39;: 1e-06}), &#39;attn_v_ADD&#39;: (&#39;FLOAT&#39;, {&#39;default&#39;: 0.0, &#39;min&#39;: -2.0, &#39;max&#39;: 2.0, &#39;step&#39;: 1e-06}), &#39;attn_out_weight_ADD&#39;: (&#39;FLOAT&#39;, {&#39;default&#39;: 0.0, &#39;min&#39;: -2.0, &#39;max&#39;: 2.0, &#39;step&#39;: 1e-06}), &#39;attn_out_bias_ADD&#39;: (&#39;FLOAT&#39;, {&#39;default&#39;: 0.0, &#39;min&#39;: -2.0, &#39;max&#39;: 2.0, &#39;step&#39;: 1e-06}), &#39;other_ADD&#39;: (&#39;FLOAT&#39;, {&#39;default&#39;: 0.0, &#39;min&#39;: -2.0, &#39;max&#39;: 2.0, &#39;step&#39;: 1e-06}), &#39;print_adjustment&#39;: (&#39;BOOLEAN&#39;, {&#39;default&#39;: False})}, &#39;optional&#39;: {&#39;prev_weight_adjust&#39;: (&#39;WEIGHT_ADJUST&#39;,)}}</span></span>
<span class="line"><span>    RETURN_TYPES = (&#39;WEIGHT_ADJUST&#39;,)</span></span>
<span class="line"><span>    CATEGORY = &#39;Animate Diff 🎭🅐🅓/ad settings/weight adjust&#39;</span></span>
<span class="line"><span>    FUNCTION = &#39;get_weight_adjust&#39;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    def get_weight_adjust(self, pe_ADD: float, attn_ADD: float, attn_q_ADD: float, attn_k_ADD: float, attn_v_ADD: float, attn_out_weight_ADD: float, attn_out_bias_ADD: float, other_ADD: float, print_adjustment: bool, prev_weight_adjust: AdjustGroup=None):</span></span>
<span class="line"><span>        if prev_weight_adjust is None:</span></span>
<span class="line"><span>            prev_weight_adjust = AdjustGroup()</span></span>
<span class="line"><span>        prev_weight_adjust = prev_weight_adjust.clone()</span></span>
<span class="line"><span>        adjust = AdjustWeight(pe_ADD=pe_ADD, attn_ADD=attn_ADD, attn_q_ADD=attn_q_ADD, attn_k_ADD=attn_k_ADD, attn_v_ADD=attn_v_ADD, attn_out_weight_ADD=attn_out_weight_ADD, attn_out_bias_ADD=attn_out_bias_ADD, other_ADD=other_ADD, print_adjustment=print_adjustment)</span></span>
<span class="line"><span>        prev_weight_adjust.add(adjust)</span></span>
<span class="line"><span>        return (prev_weight_adjust,)</span></span></code></pre></div>`,14),s=[l];function o(p,_,d,u,D,A){return e(),a("div",null,s)}const c=t(n,[["render",o]]);export{h as __pageData,c as default};
